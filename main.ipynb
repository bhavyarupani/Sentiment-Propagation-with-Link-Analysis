{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install pandas\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "dataset_url = \"http://snap.stanford.edu/data/amazon/productGraph/kcore_5.json.gz\"\n",
    "save_path = \"kcore_5.json.gz\"\n",
    "\n",
    "# Download the 5-core dataset\n",
    "urllib.request.urlretrieve(dataset_url, save_path)\n",
    "\n",
    "print(\"Dataset downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtextblob\u001b[39;00m \u001b[39mimport\u001b[39;00m TextBlob\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msentiment\u001b[39;00m \u001b[39mimport\u001b[39;00m SentimentIntensityAnalyzer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import gzip\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          reviewerID        asin               reviewerName helpful  \\\n",
      "0      ACNGUPJ3A3TM9  0000013714                        GCM  [0, 0]   \n",
      "1     A2SUAM1J3GNN3B  0000013714                J. McDonald  [2, 3]   \n",
      "2      APOZ15IEYQRRR  0000013714                  maewest64  [0, 0]   \n",
      "3      AYEDW3BFK53XK  0000013714                      Missb  [0, 0]   \n",
      "4     A1KLCGLCXYP1U1  0000013714        Paul L \"Paul Lytle\"  [0, 0]   \n",
      "...              ...         ...                        ...     ...   \n",
      "9995  A32UCGRPBE03R3  0002558033                  Greyhound  [1, 1]   \n",
      "9996   AIMQH7610W8NK  0002558033             jared matthews  [4, 5]   \n",
      "9997   AVTCCT91Q6O5N  0002558033  Jason A. Carson \"Tai Guy\"  [1, 4]   \n",
      "9998   AYI3EMZAXS6E8  0002558033                 Jason Mack  [0, 0]   \n",
      "9999  A13L24Q3OKM0VK  0002558033                Juan Castro  [0, 8]   \n",
      "\n",
      "                                             reviewText  overall  \\\n",
      "0     We use this type of hymnal at church.  I was l...      4.0   \n",
      "1     I bought this for my husband who plays the pia...      5.0   \n",
      "2     This is a large size hymn book which is great ...      5.0   \n",
      "3     We use this hymn book at the mission.  It has ...      5.0   \n",
      "4     One review advised this book was large print, ...      3.0   \n",
      "...                                                 ...      ...   \n",
      "9995  I bought this book expecting it to be about SH...      3.0   \n",
      "9996  I was looking for an advanced urban survival g...      1.0   \n",
      "9997  This book is great as a condensed version of t...      5.0   \n",
      "9998  This book should be required reading for every...      5.0   \n",
      "9999  If you go out on a regular basis, or feel like...      5.0   \n",
      "\n",
      "                               summary  unixReviewTime   reviewTime  \n",
      "0                          Nice Hymnal      1386028800   12 3, 2013  \n",
      "1               Heavenly Highway Hymns      1252800000  09 13, 2009  \n",
      "2                    Awesome Hymn Book      1362787200   03 9, 2013  \n",
      "3     Hand Clapping Toe Tapping Oldies      1325462400   01 2, 2012  \n",
      "4                           Misleading      1376092800  08 10, 2013  \n",
      "...                                ...             ...          ...  \n",
      "9995                SAS Urban Survival      1331078400   03 7, 2012  \n",
      "9996                  Complete Garbage      1357948800  01 12, 2013  \n",
      "9997                   Pocket Survival      1299369600   03 6, 2011  \n",
      "9998        Should be required reading      1395792000  03 26, 2014  \n",
      "9999     A must if you're adventureous      1233360000  01 31, 2009  \n",
      "\n",
      "[10000 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "data_file = \"kcore_5.json\\kcore_5.json\"\n",
    "num_lines = 10000\n",
    "\n",
    "# Create an empty list to store the JSON objects\n",
    "json_data = []\n",
    "\n",
    "# Read the JSON file and load the JSON objects\n",
    "with open(data_file, \"r\") as f:\n",
    "    for _ in range(num_lines):\n",
    "        line = f.readline()\n",
    "        json_obj = json.loads(line)\n",
    "        json_data.append(json_obj)\n",
    "\n",
    "# Create a DataFrame from the JSON data\n",
    "df = pd.DataFrame(json_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACNGUPJ3A3TM9</td>\n",
       "      <td>0000013714</td>\n",
       "      <td>GCM</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>We use this type of hymnal at church.  I was l...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Nice Hymnal</td>\n",
       "      <td>1386028800</td>\n",
       "      <td>12 3, 2013</td>\n",
       "      <td>use type hymnal church looking one however n't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2SUAM1J3GNN3B</td>\n",
       "      <td>0000013714</td>\n",
       "      <td>J. McDonald</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>I bought this for my husband who plays the pia...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Heavenly Highway Hymns</td>\n",
       "      <td>1252800000</td>\n",
       "      <td>09 13, 2009</td>\n",
       "      <td>bought husband play piano wonderful time playi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APOZ15IEYQRRR</td>\n",
       "      <td>0000013714</td>\n",
       "      <td>maewest64</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is a large size hymn book which is great ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Awesome Hymn Book</td>\n",
       "      <td>1362787200</td>\n",
       "      <td>03 9, 2013</td>\n",
       "      <td>large size hymn book great able see song note ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AYEDW3BFK53XK</td>\n",
       "      <td>0000013714</td>\n",
       "      <td>Missb</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>We use this hymn book at the mission.  It has ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hand Clapping Toe Tapping Oldies</td>\n",
       "      <td>1325462400</td>\n",
       "      <td>01 2, 2012</td>\n",
       "      <td>use hymn book mission well-loved oldie time go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1KLCGLCXYP1U1</td>\n",
       "      <td>0000013714</td>\n",
       "      <td>Paul L \"Paul Lytle\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>One review advised this book was large print, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Misleading</td>\n",
       "      <td>1376092800</td>\n",
       "      <td>08 10, 2013</td>\n",
       "      <td>one review advised book large print however n'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin         reviewerName helpful  \\\n",
       "0   ACNGUPJ3A3TM9  0000013714                  GCM  [0, 0]   \n",
       "1  A2SUAM1J3GNN3B  0000013714          J. McDonald  [2, 3]   \n",
       "2   APOZ15IEYQRRR  0000013714            maewest64  [0, 0]   \n",
       "3   AYEDW3BFK53XK  0000013714                Missb  [0, 0]   \n",
       "4  A1KLCGLCXYP1U1  0000013714  Paul L \"Paul Lytle\"  [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  We use this type of hymnal at church.  I was l...      4.0   \n",
       "1  I bought this for my husband who plays the pia...      5.0   \n",
       "2  This is a large size hymn book which is great ...      5.0   \n",
       "3  We use this hymn book at the mission.  It has ...      5.0   \n",
       "4  One review advised this book was large print, ...      3.0   \n",
       "\n",
       "                            summary  unixReviewTime   reviewTime  \\\n",
       "0                       Nice Hymnal      1386028800   12 3, 2013   \n",
       "1            Heavenly Highway Hymns      1252800000  09 13, 2009   \n",
       "2                 Awesome Hymn Book      1362787200   03 9, 2013   \n",
       "3  Hand Clapping Toe Tapping Oldies      1325462400   01 2, 2012   \n",
       "4                        Misleading      1376092800  08 10, 2013   \n",
       "\n",
       "                                 preprocessed_review  \n",
       "0  use type hymnal church looking one however n't...  \n",
       "1  bought husband play piano wonderful time playi...  \n",
       "2  large size hymn book great able see song note ...  \n",
       "3  use hymn book mission well-loved oldie time go...  \n",
       "4  one review advised book large print however n'...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a preprocessed text\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the review texts\n",
    "df['preprocessed_review'] = df['reviewText'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       00  000  0001714422  000th   01  016   02   03  0394826922   04  ...  \\\n",
      "0     0.0  0.0         0.0    0.0  0.0  0.0  0.0  0.0         0.0  0.0  ...   \n",
      "1     0.0  0.0         0.0    0.0  0.0  0.0  0.0  0.0         0.0  0.0  ...   \n",
      "2     0.0  0.0         0.0    0.0  0.0  0.0  0.0  0.0         0.0  0.0  ...   \n",
      "3     0.0  0.0         0.0    0.0  0.0  0.0  0.0  0.0         0.0  0.0  ...   \n",
      "4     0.0  0.0         0.0    0.0  0.0  0.0  0.0  0.0         0.0  0.0  ...   \n",
      "...   ...  ...         ...    ...  ...  ...  ...  ...         ...  ...  ...   \n",
      "9995  0.0  0.0         0.0    0.0  0.0  0.0  0.0  0.0         0.0  0.0  ...   \n",
      "9996  0.0  0.0         0.0    0.0  0.0  0.0  0.0  0.0         0.0  0.0  ...   \n",
      "9997  0.0  0.0         0.0    0.0  0.0  0.0  0.0  0.0         0.0  0.0  ...   \n",
      "9998  0.0  0.0         0.0    0.0  0.0  0.0  0.0  0.0         0.0  0.0  ...   \n",
      "9999  0.0  0.0         0.0    0.0  0.0  0.0  0.0  0.0         0.0  0.0  ...   \n",
      "\n",
      "      zoo  zookeeper  zoologist  zoom  zoomed  zoos  zzzts  zzzzz  \\\n",
      "0     0.0        0.0        0.0   0.0     0.0   0.0    0.0    0.0   \n",
      "1     0.0        0.0        0.0   0.0     0.0   0.0    0.0    0.0   \n",
      "2     0.0        0.0        0.0   0.0     0.0   0.0    0.0    0.0   \n",
      "3     0.0        0.0        0.0   0.0     0.0   0.0    0.0    0.0   \n",
      "4     0.0        0.0        0.0   0.0     0.0   0.0    0.0    0.0   \n",
      "...   ...        ...        ...   ...     ...   ...    ...    ...   \n",
      "9995  0.0        0.0        0.0   0.0     0.0   0.0    0.0    0.0   \n",
      "9996  0.0        0.0        0.0   0.0     0.0   0.0    0.0    0.0   \n",
      "9997  0.0        0.0        0.0   0.0     0.0   0.0    0.0    0.0   \n",
      "9998  0.0        0.0        0.0   0.0     0.0   0.0    0.0    0.0   \n",
      "9999  0.0        0.0        0.0   0.0     0.0   0.0    0.0    0.0   \n",
      "\n",
      "      zzzzzzzzzzzz  zzzzzzzzzzzzz  \n",
      "0              0.0            0.0  \n",
      "1              0.0            0.0  \n",
      "2              0.0            0.0  \n",
      "3              0.0            0.0  \n",
      "4              0.0            0.0  \n",
      "...            ...            ...  \n",
      "9995           0.0            0.0  \n",
      "9996           0.0            0.0  \n",
      "9997           0.0            0.0  \n",
      "9998           0.0            0.0  \n",
      "9999           0.0            0.0  \n",
      "\n",
      "[10000 rows x 26569 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Assuming you already have a DataFrame called 'df' with a column named 'preprocessed_review'\n",
    "\n",
    "# Initialize the TF-IDF vectorizer.\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the preprocessed reviews\n",
    "tfidf_matrix = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the feature names (words) from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Print the TF-IDF DataFrame\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextBlob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m sentiment\n\u001b[0;32m      6\u001b[0m \u001b[39m# Apply sentiment analysis to the preprocessed reviews\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39msentiment_score\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mpreprocessed_review\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(analyze_sentiment)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Classify the sentiment based on the sentiment score\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39msentiment_score\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m score: \u001b[39m'\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m score \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mNegative\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m score \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mNeutral\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Sentiment-Propagation-with-Link-Analysis\\.conda\\Lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32md:\\Sentiment-Propagation-with-Link-Analysis\\.conda\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32md:\\Sentiment-Propagation-with-Link-Analysis\\.conda\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32md:\\Sentiment-Propagation-with-Link-Analysis\\.conda\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m, in \u001b[0;36manalyze_sentiment\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39manalyze_sentiment\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     blob \u001b[39m=\u001b[39m TextBlob(text)\n\u001b[0;32m      3\u001b[0m     sentiment \u001b[39m=\u001b[39m blob\u001b[39m.\u001b[39msentiment\u001b[39m.\u001b[39mpolarity\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m sentiment\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TextBlob' is not defined"
     ]
    }
   ],
   "source": [
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    return sentiment\n",
    "\n",
    "# Apply sentiment analysis to the preprocessed reviews\n",
    "df['sentiment_score'] = df['preprocessed_review'].apply(analyze_sentiment)\n",
    "\n",
    "# Classify the sentiment based on the sentiment score\n",
    "df['sentiment'] = df['sentiment_score'].apply(lambda score: 'Positive' if score > 0 else 'Negative' if score < 0 else 'Neutral')\n",
    "\n",
    "# Print the DataFrame with sentiment scores and classifications\n",
    "print(df[['preprocessed_review', 'sentiment_score', 'sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
