{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install pandas\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "dataset_url = \"http://snap.stanford.edu/data/amazon/productGraph/kcore_5.json.gz\"\n",
    "save_path = \"kcore_5.json.gz\"\n",
    "\n",
    "# Download the 5-core dataset\n",
    "urllib.request.urlretrieve(dataset_url, save_path)\n",
    "\n",
    "print(\"Dataset downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import gzip\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          reviewerID        asin               reviewerName helpful  \\\n",
      "0      ACNGUPJ3A3TM9  0000013714                        GCM  [0, 0]   \n",
      "1     A2SUAM1J3GNN3B  0000013714                J. McDonald  [2, 3]   \n",
      "2      APOZ15IEYQRRR  0000013714                  maewest64  [0, 0]   \n",
      "3      AYEDW3BFK53XK  0000013714                      Missb  [0, 0]   \n",
      "4     A1KLCGLCXYP1U1  0000013714        Paul L \"Paul Lytle\"  [0, 0]   \n",
      "...              ...         ...                        ...     ...   \n",
      "9995  A32UCGRPBE03R3  0002558033                  Greyhound  [1, 1]   \n",
      "9996   AIMQH7610W8NK  0002558033             jared matthews  [4, 5]   \n",
      "9997   AVTCCT91Q6O5N  0002558033  Jason A. Carson \"Tai Guy\"  [1, 4]   \n",
      "9998   AYI3EMZAXS6E8  0002558033                 Jason Mack  [0, 0]   \n",
      "9999  A13L24Q3OKM0VK  0002558033                Juan Castro  [0, 8]   \n",
      "\n",
      "                                             reviewText  overall  \\\n",
      "0     We use this type of hymnal at church.  I was l...      4.0   \n",
      "1     I bought this for my husband who plays the pia...      5.0   \n",
      "2     This is a large size hymn book which is great ...      5.0   \n",
      "3     We use this hymn book at the mission.  It has ...      5.0   \n",
      "4     One review advised this book was large print, ...      3.0   \n",
      "...                                                 ...      ...   \n",
      "9995  I bought this book expecting it to be about SH...      3.0   \n",
      "9996  I was looking for an advanced urban survival g...      1.0   \n",
      "9997  This book is great as a condensed version of t...      5.0   \n",
      "9998  This book should be required reading for every...      5.0   \n",
      "9999  If you go out on a regular basis, or feel like...      5.0   \n",
      "\n",
      "                               summary  unixReviewTime   reviewTime  \n",
      "0                          Nice Hymnal      1386028800   12 3, 2013  \n",
      "1               Heavenly Highway Hymns      1252800000  09 13, 2009  \n",
      "2                    Awesome Hymn Book      1362787200   03 9, 2013  \n",
      "3     Hand Clapping Toe Tapping Oldies      1325462400   01 2, 2012  \n",
      "4                           Misleading      1376092800  08 10, 2013  \n",
      "...                                ...             ...          ...  \n",
      "9995                SAS Urban Survival      1331078400   03 7, 2012  \n",
      "9996                  Complete Garbage      1357948800  01 12, 2013  \n",
      "9997                   Pocket Survival      1299369600   03 6, 2011  \n",
      "9998        Should be required reading      1395792000  03 26, 2014  \n",
      "9999     A must if you're adventureous      1233360000  01 31, 2009  \n",
      "\n",
      "[10000 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "data_file = \"kcore_5.json\\kcore_5.json\"\n",
    "num_lines = 10000\n",
    "\n",
    "# Create an empty list to store the JSON objects\n",
    "json_data = []\n",
    "\n",
    "# Read the JSON file and load the JSON objects\n",
    "with open(data_file, \"r\") as f:\n",
    "    for _ in range(num_lines):\n",
    "        line = f.readline()\n",
    "        json_obj = json.loads(line)\n",
    "        json_data.append(json_obj)\n",
    "\n",
    "# Create a DataFrame from the JSON data\n",
    "df = pd.DataFrame(json_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACNGUPJ3A3TM9</td>\n",
       "      <td>0000013714</td>\n",
       "      <td>GCM</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>We use this type of hymnal at church.  I was l...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Nice Hymnal</td>\n",
       "      <td>1386028800</td>\n",
       "      <td>12 3, 2013</td>\n",
       "      <td>use type hymnal church looking one however n't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2SUAM1J3GNN3B</td>\n",
       "      <td>0000013714</td>\n",
       "      <td>J. McDonald</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>I bought this for my husband who plays the pia...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Heavenly Highway Hymns</td>\n",
       "      <td>1252800000</td>\n",
       "      <td>09 13, 2009</td>\n",
       "      <td>bought husband play piano wonderful time playi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APOZ15IEYQRRR</td>\n",
       "      <td>0000013714</td>\n",
       "      <td>maewest64</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is a large size hymn book which is great ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Awesome Hymn Book</td>\n",
       "      <td>1362787200</td>\n",
       "      <td>03 9, 2013</td>\n",
       "      <td>large size hymn book great able see song note ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AYEDW3BFK53XK</td>\n",
       "      <td>0000013714</td>\n",
       "      <td>Missb</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>We use this hymn book at the mission.  It has ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hand Clapping Toe Tapping Oldies</td>\n",
       "      <td>1325462400</td>\n",
       "      <td>01 2, 2012</td>\n",
       "      <td>use hymn book mission well-loved oldie time go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1KLCGLCXYP1U1</td>\n",
       "      <td>0000013714</td>\n",
       "      <td>Paul L \"Paul Lytle\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>One review advised this book was large print, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Misleading</td>\n",
       "      <td>1376092800</td>\n",
       "      <td>08 10, 2013</td>\n",
       "      <td>one review advised book large print however n'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin         reviewerName helpful  \\\n",
       "0   ACNGUPJ3A3TM9  0000013714                  GCM  [0, 0]   \n",
       "1  A2SUAM1J3GNN3B  0000013714          J. McDonald  [2, 3]   \n",
       "2   APOZ15IEYQRRR  0000013714            maewest64  [0, 0]   \n",
       "3   AYEDW3BFK53XK  0000013714                Missb  [0, 0]   \n",
       "4  A1KLCGLCXYP1U1  0000013714  Paul L \"Paul Lytle\"  [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  We use this type of hymnal at church.  I was l...      4.0   \n",
       "1  I bought this for my husband who plays the pia...      5.0   \n",
       "2  This is a large size hymn book which is great ...      5.0   \n",
       "3  We use this hymn book at the mission.  It has ...      5.0   \n",
       "4  One review advised this book was large print, ...      3.0   \n",
       "\n",
       "                            summary  unixReviewTime   reviewTime  \\\n",
       "0                       Nice Hymnal      1386028800   12 3, 2013   \n",
       "1            Heavenly Highway Hymns      1252800000  09 13, 2009   \n",
       "2                 Awesome Hymn Book      1362787200   03 9, 2013   \n",
       "3  Hand Clapping Toe Tapping Oldies      1325462400   01 2, 2012   \n",
       "4                        Misleading      1376092800  08 10, 2013   \n",
       "\n",
       "                                 preprocessed_review  \n",
       "0  use type hymnal church looking one however n't...  \n",
       "1  bought husband play piano wonderful time playi...  \n",
       "2  large size hymn book great able see song note ...  \n",
       "3  use hymn book mission well-loved oldie time go...  \n",
       "4  one review advised book large print however n'...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a preprocessed text\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the review texts\n",
    "df['preprocessed_review'] = df['reviewText'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m tfidf_matrix \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(df[\u001b[39m'\u001b[39m\u001b[39mpreprocessed_review\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[39m# Get the feature names (words) from the vectorizer\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m feature_names \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mget_feature_names()\n\u001b[0;32m     13\u001b[0m \u001b[39m# Convert the TF-IDF matrix to a DataFrame\u001b[39;00m\n\u001b[0;32m     14\u001b[0m tfidf_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(tfidf_matrix\u001b[39m.\u001b[39mtoarray(), columns\u001b[39m=\u001b[39mfeature_names)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Assuming you already have a DataFrame called 'df' with a column named 'preprocessed_review'\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the preprocessed reviews\n",
    "tfidf_matrix = vectorizer.fit_transform(df['preprocessed_review'])\n",
    "\n",
    "# Get the feature names (words) from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Print the TF-IDF DataFrame\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the preprocessed review texts\n",
    "tfidf_matrix = vectorizer.fit_transform([preprocessed_review1, preprocessed_review2])\n",
    "\n",
    "# Sentiment analysis using VADER SentimentIntensityAnalyzer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment scores for the original review texts\n",
    "sentiment_scores1 = analyzer.polarity_scores(review1)\n",
    "sentiment_scores2 = analyzer.polarity_scores(review2)\n",
    "\n",
    "# Accessing the compound sentiment scores\n",
    "sentiment_score1 = sentiment_scores1['compound']\n",
    "sentiment_score2 = sentiment_scores2['compound']\n",
    "\n",
    "print(\"Sentiment score for review 1:\", sentiment_score1)\n",
    "print(\"Sentiment score for review 2:\", sentiment_score2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
